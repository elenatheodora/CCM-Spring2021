{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Neural networks - Part B (45 points)\n",
    "## Gradient descent for simple two and three layer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday, Feb. 22, 2021.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment implements the gradient descent algorithm for a simple artificial neuron. The second part implements backpropagation for a simple network with one hidden unit.\n",
    "\n",
    "In the first part, the neuron will learn to compute logical OR. The neuron model and logical OR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_OR.jpeg\" style=\"width: 350px;\"/>\n",
    "\n",
    "This assignment requires some basic PyTorch knowledge. You can review your notes from lab and also two basic [PyTorch tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), \"What is PyTorch?\" and \"Autograd\", which should have the basics you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `torch.tensor` objects for representing the data matrix `D` with targets `Y_or` (for the logical OR function). Each row of `D` is a different data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "D = np.zeros((4,2),dtype=float)\n",
    "D[0,:] = [0.,0.]\n",
    "D[1,:] = [0.,1.]\n",
    "D[2,:] = [1.,0.]\n",
    "D[3,:] = [1.,1.]\n",
    "D = torch.tensor(D,dtype=torch.float)\n",
    "Y_or = torch.tensor([0.,1.,1.,1.])\n",
    "N = D.shape[0] # number of input patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial neuron operates as follows. Given an input vector $x$, the net input ($\\textbf{net}$) to the neuron is computed as follows\n",
    "\n",
    "$$ \\textbf{net} = \\sum_i x_i w_i + b,$$\n",
    "\n",
    "for weights $w_i$ and bias $b$. The activation function $g(\\textbf{net})$ is the logistic function\n",
    "\n",
    "$$ g(\\textbf{net}) = \\frac{1}{1+e^{-\\textbf{net}}},$$\n",
    "\n",
    "which is used to compute the predicted output $\\hat{y} = g(\\textbf{net})$. Finally, the loss (squared error) for a particular pattern $x$ is defined as \n",
    "\n",
    "$$ E(w,b) = (\\hat{y}-y)^2,$$\n",
    "\n",
    "where the target output is $y$. **Your main task is to manually compute the gradients of the loss $E$ with respect to the neuron parameters:**\n",
    "\n",
    "$$\\frac{\\partial E(w,b)}{\\partial w}, \\frac{\\partial E(w,b)}{\\partial b}.$$\n",
    "\n",
    "By manually, we mean to program the gradient computation directly, using the formulas discussed in class. This is in contrast to using PyTorch's `autograd` (Automatric differentiation) that computes the gradient automatically, as discussed in class, lab, and in the PyTorch tutorial (e.g., `loss.backward()`). First, let's write the activation function and the loss in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_logistic(net):\n",
    "    return 1. / (1.+torch.exp(-net))\n",
    "\n",
    "def loss(yhat,y):\n",
    "    return (yhat-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll also write two functions for examining the internal operations of the neuron, and the gradients of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_forward(x,yhat,y):\n",
    "    # Examine network's prediction for input x\n",
    "    print(' Input: ',end='')\n",
    "    print(x.numpy())\n",
    "    print(' Output: ' + str(round(yhat.item(),3)))\n",
    "    print(' Target: ' + str(y.item()))\n",
    "\n",
    "def print_grad(grad_w,grad_b):\n",
    "    # Examine gradients\n",
    "    print('  d_loss / d_w = ',end='')\n",
    "    print(grad_w)\n",
    "    print('  d_loss / d_b = ',end='')\n",
    "    print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive in and begin the implementation of stochastic gradient descent. We'll initialize our parameters $w$ and $b$ randomly, and proceed through a series of epochs of training. Each epoch involves visiting the four training patterns in random order, and updating the parameters after each presentation of an input pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (10 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code to manually compute the gradient in closed form.\n",
    "    <ul>\n",
    "        <li>See lecture slides for the equation for the gradient for the weights w.</li>\n",
    "        <li>Derive (or reason) to get the equation for the gradient for bias b.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (5 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code for the weight and bias update rule for gradient descent.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the neuron's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2) # [size 2] tensor\n",
    "b = torch.randn(1) # [size 1] tensor\n",
    "w.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x = D[p,:] # input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x,w)+b\n",
    "        yhat = g_logistic(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.         \n",
    "            raise Exception('Replace with your code.')                      \n",
    "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            #    w -=   ....\n",
    "            #    b -=   ....\n",
    "            raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (logistic activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change the activation function to \"tanh\" from the \"logistic\" function, such that $g(\\textbf{net}) = \\tanh(\\textbf{net})$. Here is an implementation of tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_tanh(x):\n",
    "    return (torch.exp(x) - torch.exp(-x))/(torch.exp(x) + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the tanh function is as follows:\n",
    "\n",
    "$$\\frac{\\partial g(\\textbf{net})}{\\partial \\textbf{net}}= \\frac{\\partial \\tanh(\\textbf{net})}{\\partial \\textbf{net}} = 1.0 - (\\tanh(\\textbf{net}))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 3 (5 points) </h3>\n",
    "<br>\n",
    "Just as before, fill in the missing code fragments for implementing gradient descent. This time we are using the tanh activation function. Be sure to change your gradient calculation to reflect the new activation function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2) # [size 2] tensor\n",
    "b = torch.randn(1) # [size 1] tensor\n",
    "w.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x = D[p,:] # input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x,w)+b\n",
    "        yhat = g_tanh(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the graident computation, which we don't want.\n",
    "            raise Exception('Replace with your code.')                       \n",
    "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            #    w -=   ....\n",
    "            #    b -=   ....\n",
    "            raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (tanh activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we have a simple multi-layer network with two input neurons, one hidden neuron, and one output neuron. Both the hidden and output unit should use the logistic activation function. We will learn to compute logical XOR. The network and logical XOR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_XOR.jpeg\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 4 (15 points) </h3>\n",
    "<br>\n",
    "You will implement backpropagation for this simple network. In the code below, you have several parts to fill in. First, define the forward pass to compute the output `yhat` from the input `x`. Second, fill in code to manually compute the gradients for all five weights w and two biases b in closed form. Third, fill in the code for updating the biases and weights.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the network's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data D and labels y for xor\n",
    "D = np.zeros((4,2),dtype=float)\n",
    "D[0,:] = [0.,0.]\n",
    "D[1,:] = [0.,1.]\n",
    "D[2,:] = [1.,0.]\n",
    "D[3,:] = [1.,1.]\n",
    "D = torch.tensor(D,dtype=torch.float)\n",
    "Y_xor = torch.tensor([0.,1.,1.,0.])\n",
    "N = D.shape[0] # number of input patterns\n",
    "\n",
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w_34 = torch.randn(2) # [size 2] tensor representing [w_3,w_4]\n",
    "w_012 = torch.randn(3) # [size 3] tensor representing [w_0,w_1,w_2]\n",
    "b_0 = torch.randn(1) # [size 1] tensor\n",
    "b_1 = torch.randn(1) # [size 1] tensor\n",
    "w_34.requires_grad=True\n",
    "w_012.requires_grad=True\n",
    "b_0.requires_grad=True\n",
    "b_1.requires_grad=True\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 8000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x = D[p,:] # input pattern\n",
    "        \n",
    "        # Compute the output of hidden neuron h\n",
    "        # e.g., two lines like the following\n",
    "        #  net_h = ...\n",
    "        #  h = ...\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        raise Exception('Replace with your code.')                  \n",
    "        \n",
    "        # Compute the output of neuron yhat\n",
    "        # e.g., two lines like the following\n",
    "        #  net_y = ...\n",
    "        #  yhat = ...\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        raise Exception('Replace with your code.')                     \n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_xor[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  should include at least these 4 lines (helper lines may be useful)\n",
    "            #    w_34_grad = ...  \n",
    "            #    b_0_grad = ...\n",
    "            #    w_012_grad = ...\n",
    "            #    b_1_grad = ...\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.\n",
    "            raise Exception('Replace with your code.')                      \n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34_grad.numpy(),b_0_grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012_grad.numpy(),b_1_grad.numpy())\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34.grad.numpy(),b_0.grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012.grad.numpy(),b_1.grad.numpy())\n",
    "            print(\"\")\n",
    "        w_34.grad.zero_() # clear PyTorch's gradient\n",
    "        b_0.grad.zero_()\n",
    "        w_012.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            # Four lines of the form\n",
    "            # w_34 -= ...\n",
    "            # b_0 -= ...\n",
    "            # w_012 -= ...\n",
    "            # b_1 -= ...\n",
    "            raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (XOR)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 5 (10 points) </h3>\n",
    "<br>\n",
    "After running your XOR network, print the values of the learned weights and biases. Your job now is to describe the solution that the network has learned. How does it work? Walk through each input pattern to describe how the network computes the right answer (if it does). See discussion in lecture for an example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE GOES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
